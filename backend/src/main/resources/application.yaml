server:
  port: 8080

spring:
  application:
    name: backend
  ai:
    chat:
      client:
        enabled: false
    openai:
      api-key: ${OPENAI_API_KEY:demo-openai-key}
      base-url: ${OPENAI_BASE_URL:https://api.openai.com/v1}
      client:
        read-timeout: PT2M
      chat:
        options:
          model: ${OPENAI_DEFAULT_MODEL:gpt-4o-mini}
          temperature: ${OPENAI_TEMPERATURE:0.7}
          top-p: ${OPENAI_TOP_P:1.0}
          max-tokens: ${OPENAI_MAX_TOKENS:1024}
      log-request: true
      log-response: true
    zhipuai:
      api-key: ${ZHIPU_API_KEY:demo-zhipu-key}
      base-url: ${ZHIPU_BASE_URL:https://api.z.ai}
      client:
        read-timeout: PT2M
      chat:
        completions-path: ${ZHIPU_COMPLETIONS_PATH:/api/paas/v4/chat/completions}
        options:
          model: ${ZHIPU_DEFAULT_MODEL:glm-4.6}
          temperature: ${ZHIPU_TEMPERATURE:0.7}
          top-p: ${ZHIPU_TOP_P:0.95}
          max-tokens: ${ZHIPU_MAX_TOKENS:1024}

logging:
  level:
    com.aiadvent.backend.chat: DEBUG
    org.springframework.ai: DEBUG
    org.springframework.web: INFO
    org.springframework.ai.chat.client.advisor: DEBUG

app:
  chat:
    default-provider: zhipu
    providers:
      zhipu:
        type: ZHIPUAI
        display-name: "zhipu.ai"
        base-url: ${ZHIPU_BASE_URL:https://api.z.ai}
        api-key: ${ZHIPU_API_KEY:demo-zhipu-key}
        completions-path: ${ZHIPU_COMPLETIONS_PATH:/api/paas/v4/chat/completions}
        timeout: ${ZHIPU_TIMEOUT:PT2M}
        temperature: ${ZHIPU_TEMPERATURE:0.7}
        top-p: ${ZHIPU_TOP_P:0.95}
        max-tokens: ${ZHIPU_MAX_TOKENS:1024}
        default-model: ${ZHIPU_DEFAULT_MODEL:glm-4.6}
        retry:
          attempts: ${ZHIPU_RETRY_ATTEMPTS:3}
          initial-delay: ${ZHIPU_RETRY_INITIAL_DELAY:250ms}
          multiplier: ${ZHIPU_RETRY_MULTIPLIER:2.0}
          retryable-statuses: ${ZHIPU_RETRY_STATUS_CODES:429,500,502,503,504}
        models:
          "[glm-4.6]":
            display-name: "GLM-4.6"
            tier: pro
            pricing:
              input-per-1k-tokens: 0.0006
              output-per-1k-tokens: 0.0022
          "[glm-4.5]":
            display-name: "GLM-4.5"
            tier: standard
            pricing:
              input-per-1k-tokens: 0.00035
              output-per-1k-tokens: 0.00155
          "[glm-4.5-air]":
            display-name: "GLM-4.5 Air"
            tier: budget
            pricing:
              input-per-1k-tokens: 0.0002
              output-per-1k-tokens: 0.0011
      openai:
        type: OPENAI
        display-name: "OpenAI"
        base-url: ${OPENAI_BASE_URL:https://api.openai.com/v1}
        api-key: ${OPENAI_API_KEY:demo-openai-key}
        timeout: ${OPENAI_TIMEOUT:PT2M}
        temperature: ${OPENAI_TEMPERATURE:0.7}
        top-p: ${OPENAI_TOP_P:1.0}
        max-tokens: ${OPENAI_MAX_TOKENS:1024}
        default-model: ${OPENAI_DEFAULT_MODEL:gpt-4o-mini}
        retry:
          attempts: ${OPENAI_RETRY_ATTEMPTS:3}
          initial-delay: ${OPENAI_RETRY_INITIAL_DELAY:250ms}
          multiplier: ${OPENAI_RETRY_MULTIPLIER:2.0}
          retryable-statuses: ${OPENAI_RETRY_STATUS_CODES:429,500,502,503,504}
        models:
          gpt-4o-mini:
            display-name: "GPT-4o Mini"
            tier: budget
            pricing:
              input-per-1k-tokens: 0.00015
              output-per-1k-tokens: 0.0006
          gpt-4o:
            display-name: "GPT-4o"
            tier: pro
            pricing:
              input-per-1k-tokens: 0.0012
              output-per-1k-tokens: 0.0032
    memory:
      window-size: ${CHAT_MEMORY_WINDOW_SIZE:20}
      retention: ${CHAT_MEMORY_RETENTION:PT6H}
      cleanup-interval: ${CHAT_MEMORY_CLEANUP_INTERVAL:PT30M}
