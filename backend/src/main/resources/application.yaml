server:
  port: 8080

spring:
  application:
    name: backend
  ai:
    openai:
      base-url: ${LLM_BASE_URL:https://api.z.ai}
      api-key: ${LLM_API_KEY:}
      chat:
        completions-path: ${LLM_CHAT_COMPLETIONS_PATH:/api/paas/v4/chat/completions}
        options:
          model: ${LLM_MODEL:glm-4.6}
          temperature: ${LLM_TEMPERATURE:0.7}
          top-p: ${LLM_TOP_P:1.0}
          max-tokens: ${LLM_MAX_TOKENS:1024}
          stream: true
      log-request: true
      log-response: true

logging:
  level:
    com.aiadvent.backend.chat: DEBUG
    org.springframework.ai: DEBUG
    org.springframework.web: INFO
    org.springframework.ai.chat.client.advisor: DEBUG
#    org.springframework.web.client.RestTemplate: DEBUG
#    reactor.netty.http.client: DEBUG
#    org.springframework.ai.openai: DEBUG

app:
  chat:
    memory:
      window-size: ${CHAT_MEMORY_WINDOW_SIZE:20}
      retention: ${CHAT_MEMORY_RETENTION:PT6H}
      cleanup-interval: ${CHAT_MEMORY_CLEANUP_INTERVAL:PT30M}
