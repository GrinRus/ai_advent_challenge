server:
  port: 8080

spring:
  application:
    name: backend
  http:
    client:
      connect-timeout: ${HTTP_CLIENT_CONNECT_TIMEOUT:PT45S}
      read-timeout: ${HTTP_CLIENT_READ_TIMEOUT:PT4M}
  ai:
    tools:
      observations:
        include-content: true   # ⚠️ может содержать чувствительные данные
    chat:
      client:
        enabled: false
        observations:
          log-prompt: true      # вывести собранный prompt
      observations:
        log-prompt: true        # вывести prompt на уровне ChatModel
        log-completion: true    # вывести completion
    mcp:
      client:
        enabled: ${PERPLEXITY_MCP_ENABLED:true}
        name: perplexity-mcp-client
        request-timeout: ${PERPLEXITY_TIMEOUT_MS:120s}
        type: SYNC
        toolcallback:
          enabled: true
        annotation-scanner:
          enabled: false
        stdio:
          connections:
            perplexity:
              command: perplexity-mcp   # глобальный бинарь из /usr/local/bin
              args: []                                        # обычно без аргументов
              env:
                PERPLEXITY_API_KEY: ${PERPLEXITY_API_KEY:demo-perplexity-key}
        streamable-http:
          connections:
            agentops:
              url: ${AGENT_OPS_MCP_HTTP_BASE_URL:http://localhost:7091}
              endpoint: ${AGENT_OPS_MCP_HTTP_ENDPOINT:/mcp}
#            flowops:
#              url: ${FLOW_OPS_MCP_HTTP_BASE_URL:http://localhost:7092}
#              endpoint: ${FLOW_OPS_MCP_HTTP_ENDPOINT:/mcp}
#            insight:
#              url: ${INSIGHT_MCP_HTTP_BASE_URL:http://localhost:7093}
#              endpoint: ${INSIGHT_MCP_HTTP_ENDPOINT:/mcp}
    openai:
      api-key: ${OPENAI_API_KEY:demo-openai-key}
      base-url: ${OPENAI_BASE_URL:https://api.openai.com/v1}
      client:
        read-timeout: ${OPENAI_CLIENT_READ_TIMEOUT:${HTTP_CLIENT_READ_TIMEOUT:PT4M}}
      chat:
        options:
          model: ${OPENAI_DEFAULT_MODEL:gpt-4o-mini}
          temperature: ${OPENAI_TEMPERATURE:0.7}
          top-p: ${OPENAI_TOP_P:1.0}
          max-tokens: ${OPENAI_MAX_TOKENS:1024}
      log-request: true
      log-response: true
    zhipuai:
      api-key: ${ZHIPU_API_KEY:demo-zhipu-key}
      base-url: ${ZHIPU_BASE_URL:https://api.z.ai}
      client:
        read-timeout: ${ZHIPU_CLIENT_READ_TIMEOUT:${HTTP_CLIENT_READ_TIMEOUT:PT4M}}
      chat:
        completions-path: ${ZHIPU_COMPLETIONS_PATH:/api/paas/v4/chat/completions}
        options:
          model: ${ZHIPU_DEFAULT_MODEL:glm-4.6}
          temperature: ${ZHIPU_TEMPERATURE:0.7}
          top-p: ${ZHIPU_TOP_P:0.95}
          max-tokens: ${ZHIPU_MAX_TOKENS:1024}

management:
  tracing:
    enabled: true
    sampling:
      probability: 1.0
    logging:
      enabled: true          # выводить трейсы в логи

springdoc:
  api-docs:
    enabled: true
  swagger-ui:
    displayRequestDuration: true
    tagsSorter: alpha

logging:
  level:
    com.aiadvent: DEBUG
    com.aiadvent.backend.chat: DEBUG
    com.aiadvent.backend.mcp: DEBUG
    org.springframework.ai: DEBUG
    org.springframework.web: INFO
    ConstructorAudit: INFO

    # обязательно DEBUG для обработчика наблюдений, иначе логи не увидишь
    org.springframework.ai.model.chat.observation: DEBUG
    org.springframework.ai.chat: DEBUG

    # tool calling в целом
    org.springframework.ai.model.tool: DEBUG
    org.springframework.ai.tool: DEBUG
    org.springframework.ai.tool.method: DEBUG
    org.springframework.ai.tool.executor: DEBUG
    org.springframework.ai.tool.convert: DEBUG

    # MCP-уровень (важно для perplexity_search)
    org.springframework.ai.mcp: DEBUG
    org.springframework.ai.mcp.client: DEBUG
    org.springframework.ai.mcp.tool: DEBUG

    # чтобы видеть обмен с моделью и LoggingChatModel
    org.springframework.ai.chat.model: DEBUG
    org.springframework.ai.chat.client.advisor: DEBUG
    com.aiadvent.backend.chat.logging: DEBUG

app:
  chat:
    research:
      system-prompt: |
        You are a meticulous research analyst. Use the configured MCP tools to collect current
        information, enrich answers with concise insights, and always cite numbered sources like [1].
      structured-advice: |
        При необходимости вызывай подключенные MCP-инструменты, чтобы получить актуальные данные.
        Включай ссылки на источники в поле `details`, оформляя их в формате [n], где n соответствует номеру источника.
        Если инструмент возвращает несколько ссылок, добавь краткое пояснение каждой.
      tools:
        - code: perplexity_search
          schema-version: 1
          execution-mode: AUTO
          request-overrides:
            max_results: 8
        - code: agent_ops.list_agents
          schema-version: 1
          execution-mode: MANUAL
        - code: agent_ops.preview_dependencies
          schema-version: 1
          execution-mode: MANUAL
        - code: agent_ops.register_agent
          schema-version: 1
          execution-mode: MANUAL
        - code: flow_ops.list_flows
          schema-version: 1
          execution-mode: MANUAL
        - code: flow_ops.diff_flow_version
          schema-version: 1
          execution-mode: MANUAL
        - code: flow_ops.publish_flow
          schema-version: 1
          execution-mode: MANUAL
        - code: flow_ops.rollback_flow
          schema-version: 1
          execution-mode: MANUAL
        - code: flow_ops.validate_blueprint
          schema-version: 1
          execution-mode: MANUAL
        - code: insight.recent_sessions
          schema-version: 1
          execution-mode: MANUAL
        - code: insight.fetch_metrics
          schema-version: 1
          execution-mode: MANUAL
        - code: insight.fetch_summary
          schema-version: 1
          execution-mode: MANUAL
        - code: insight.search_memory
          schema-version: 1
          execution-mode: MANUAL
      default-tool-codes:
        - perplexity_search
    logging:
      model:
        enabled: ${CHAT_LOGGING_MODEL_ENABLED:true}
        log-completion: ${CHAT_LOGGING_MODEL_COMPLETION:true}
      tools:
        enabled: ${CHAT_LOGGING_TOOLS_ENABLED:true}
    memory:
      window-size: ${CHAT_MEMORY_WINDOW_SIZE:20}
      retention: ${CHAT_MEMORY_RETENTION:PT6H}
      cleanup-interval: ${CHAT_MEMORY_CLEANUP_INTERVAL:PT30M}
      summarization:
        enabled: ${CHAT_MEMORY_SUMMARIZATION_ENABLED:false}
        trigger-token-limit: ${CHAT_MEMORY_SUMMARIZATION_TRIGGER:12000}
        target-token-count: ${CHAT_MEMORY_SUMMARIZATION_TARGET:6000}
        model: ${CHAT_MEMORY_SUMMARIZATION_MODEL:openai:gpt-4o-mini}
        max-concurrent-summaries: ${CHAT_MEMORY_SUMMARIZATION_MAX_CONCURRENT:4}
        max-queue-size: ${CHAT_MEMORY_SUMMARIZATION_MAX_QUEUE_SIZE:100}
        retained-messages: ${CHAT_MEMORY_SUMMARY_RETAINED_MESSAGES:200}
        backfill:
          enabled: ${CHAT_MEMORY_SUMMARIZATION_BACKFILL_ENABLED:false}
          min-messages: ${CHAT_MEMORY_SUMMARIZATION_BACKFILL_MIN_MESSAGES:40}
          batch-size: ${CHAT_MEMORY_SUMMARIZATION_BACKFILL_BATCH_SIZE:25}
          max-iterations: ${CHAT_MEMORY_SUMMARIZATION_BACKFILL_MAX_ITERATIONS:20}
    default-provider: zhipu
    providers:
      zhipu:
        type: ZHIPUAI
        display-name: "zhipu.ai"
        base-url: ${ZHIPU_BASE_URL:https://api.z.ai}
        api-key: ${ZHIPU_API_KEY:demo-zhipu-key}
        completions-path: ${ZHIPU_COMPLETIONS_PATH:/api/paas/v4/chat/completions}
        timeout: ${ZHIPU_TIMEOUT:${HTTP_CLIENT_READ_TIMEOUT:PT4M}}
        temperature: ${ZHIPU_TEMPERATURE:0.7}
        top-p: ${ZHIPU_TOP_P:0.95}
        max-tokens: ${ZHIPU_MAX_TOKENS:1024}
        default-model: ${ZHIPU_DEFAULT_MODEL:glm-4.6}
        retry:
          attempts: ${ZHIPU_RETRY_ATTEMPTS:3}
          initial-delay: ${ZHIPU_RETRY_INITIAL_DELAY:250ms}
          multiplier: ${ZHIPU_RETRY_MULTIPLIER:2.0}
          retryable-statuses: ${ZHIPU_RETRY_STATUS_CODES:429,500,502,503,504}
        models:
          "[glm-4.6]":
            display-name: "GLM-4.6"
            tier: pro
            context-window: 81920
            sync-enabled: true
            pricing:
              input-per-1k-tokens: 0.0006
              output-per-1k-tokens: 0.0022
            usage:
              mode: fallback
              fallback-tokenizer: cl100k_base
          "[glm-4.5]":
            display-name: "GLM-4.5"
            tier: standard
            context-window: 65536
            sync-enabled: true
            pricing:
              input-per-1k-tokens: 0.00035
              output-per-1k-tokens: 0.00155
            usage:
              mode: fallback
              fallback-tokenizer: cl100k_base
          "[glm-4.5-air]":
            display-name: "GLM-4.5 Air"
            tier: budget
            context-window: 32768
            sync-enabled: true
            pricing:
              input-per-1k-tokens: 0.0002
              output-per-1k-tokens: 0.0011
            usage:
              mode: fallback
              fallback-tokenizer: cl100k_base
          "[glm-4-32b-0414-128k]":
            display-name: "GLM-4 32B 0414 (128K)"
            tier: flagship
            context-window: 128000
            sync-enabled: true
            streaming-enabled: false
            structured-enabled: false
            pricing:
              input-per-1k-tokens: 0.0001
              output-per-1k-tokens: 0.0001
            usage:
              mode: fallback
        fallback-tokenizer: cl100k_base
      openai:
        type: OPENAI
        display-name: "OpenAI"
        base-url: ${OPENAI_BASE_URL:https://api.openai.com/v1}
        api-key: ${OPENAI_API_KEY:demo-openai-key}
        timeout: ${OPENAI_TIMEOUT:PT2M}
        temperature: ${OPENAI_TEMPERATURE:0.7}
        top-p: ${OPENAI_TOP_P:1.0}
        max-tokens: ${OPENAI_MAX_TOKENS:1024}
        default-model: ${OPENAI_DEFAULT_MODEL:gpt-4o-mini}
        retry:
          attempts: ${OPENAI_RETRY_ATTEMPTS:3}
          initial-delay: ${OPENAI_RETRY_INITIAL_DELAY:250ms}
          multiplier: ${OPENAI_RETRY_MULTIPLIER:2.0}
          retryable-statuses: ${OPENAI_RETRY_STATUS_CODES:429,500,502,503,504}
        models:
          gpt-5-nano:
            display-name: "GPT-5 Nano"
            tier: economy
            context-window: 128000
            max-output-tokens: 8192
            use-completion-tokens: true
            sync-enabled: true
            pricing:
              input-per-1k-tokens: 0.00005
              output-per-1k-tokens: 0.0004
            usage:
              mode: native
          gpt-4o-mini:
            display-name: "GPT-4o Mini"
            tier: budget
            context-window: 128000
            max-output-tokens: 16384
            sync-enabled: true
            pricing:
              input-per-1k-tokens: 0.00015
              output-per-1k-tokens: 0.0006
            usage:
              mode: native
          gpt-4o:
            display-name: "GPT-4o"
            tier: pro
            context-window: 128000
            max-output-tokens: 16384
            sync-enabled: true
            pricing:
              input-per-1k-tokens: 0.0025
              output-per-1k-tokens: 0.01
            usage:
              mode: native
          gpt-5:
            display-name: "GPT-5"
            tier: flagship
            context-window: 200000
            max-output-tokens: 32768
            use-completion-tokens: true
            sync-enabled: true
            pricing:
              input-per-1k-tokens: 0.00125
              output-per-1k-tokens: 0.01
            usage:
              mode: native

  mcp:
    catalog:
      servers:
        perplexity:
          display-name: "Perplexity MCP"
          description: "Поисковый и research-сервер Perplexity (search / deep research)."
          tags: ["search", "research"]
          security-policy: "external-readonly"
        agentops:
          display-name: "Agent Ops MCP"
          description: "Управление каталогом агентов: регистрация, просмотр и диагностика зависимостей."
          tags: ["agents", "operations"]
          security-policy: "internal-admin"
        flowops:
          display-name: "Flow Ops MCP"
          description: "Инструменты для управления и публикации flow-definition."
          tags: ["flows", "operations"]
          security-policy: "internal-change-managed"
        insight:
          display-name: "Insight MCP"
          description: "Аналитика сессий чатов и flow: summary, поиск по памяти, метрики."
          tags: ["analytics", "observability"]
          security-policy: "internal-readonly"

  flow:
    api:
      v2-enabled: ${FLOW_API_V2_ENABLED:false}
    migration:
      cli:
        enabled: ${FLOW_MIGRATION_CLI_ENABLED:false}
        dry-run: ${FLOW_MIGRATION_CLI_DRY_RUN:true}
        include-history: ${FLOW_MIGRATION_CLI_INCLUDE_HISTORY:true}
        fail-on-error: ${FLOW_MIGRATION_CLI_FAIL_ON_ERROR:false}
    worker:
      enabled: ${FLOW_WORKER_ENABLED:true}
      poll-delay: ${FLOW_WORKER_POLL_DELAY:PT0.5S}
      max-concurrency: ${FLOW_WORKER_MAX_CONCURRENCY:1}
      worker-id-prefix: ${FLOW_WORKER_ID_PREFIX:}
    memory:
      window-size: ${CHAT_MEMORY_WINDOW_SIZE:20}
      retention: ${CHAT_MEMORY_RETENTION:PT6H}
      cleanup-interval: ${CHAT_MEMORY_CLEANUP_INTERVAL:PT30M}
    token-usage:
      default-tokenizer: ${CHAT_TOKEN_USAGE_TOKENIZER:cl100k_base}
      cache:
        enabled: ${CHAT_TOKEN_USAGE_CACHE_ENABLED:false}
        ttl: ${CHAT_TOKEN_USAGE_CACHE_TTL:PT15M}
        key-prefix: ${CHAT_TOKEN_USAGE_CACHE_PREFIX:chat:usage}
