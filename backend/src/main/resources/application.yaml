server:
  port: 8080

spring:
  application:
    name: backend
  ai:
    openai:
      base-url: ${LLM_BASE_URL:https://api.z.ai}
      api-key: ${LLM_API_KEY:}
      chat:
        completions-path: ${LLM_CHAT_COMPLETIONS_PATH:/api/paas/v4/chat/completions}
        options:
          model: ${LLM_MODEL:glm-4.6}
          temperature: ${LLM_TEMPERATURE:0.7}
          top-p: ${LLM_TOP_P:1.0}
          max-tokens: ${LLM_MAX_TOKENS:1024}
          stream: true

logging:
  level:
    com.aiadvent.backend.chat: DEBUG
    org.springframework.ai: DEBUG
