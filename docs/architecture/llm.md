# Интеграция с LLM

Проект использует Spring AI и совместимый с OpenAI протокол провайдера z.ai (`glm-4.6`). Ниже описан поток данных и ключевые настройки.

## Поток взаимодействия
1. Клиент отправляет историю диалога и новое сообщение на эндпоинт `/api/llm/chat/stream`.
2. Контроллер backend формирует запрос к Spring AI, применяет указанные опции модели.
3. Spring AI открывает стрим к провайдеру z.ai и транслирует токены обратно клиенту в виде SSE-событий (`session`, `token`, `complete`, `error`).
4. Backend сохраняет важные фрагменты истории и метаданные диалога.

## Конфигурация
- Значения по умолчанию задаются в `application.yaml` и могут переопределяться через переменные окружения.
- Основные переменные:
  - `LLM_BASE_URL`, `LLM_CHAT_COMPLETIONS_PATH`.
  - `LLM_API_KEY` (обязателен).
  - `LLM_MODEL`, `LLM_TEMPERATURE`, `LLM_TOP_P`, `LLM_MAX_TOKENS`.
- В `docker-compose.yml` переменные пробрасываются через секцию `environment`.

## Расширение и безопасность
- Планируется добавить аудит взаимодействий и тонкую настройку параметров модели per-request.
- Без дополнительной авторизации доступ открыт лишь из внутренних приложений; для внешнего доступа потребуется слой аутентификации.
- Для защиты ключей используйте секреты CI/CD и dotenv файлы (см. `docs/processes.md`).

Диаграммы последовательности и компоненты интеграции добавляйте в `docs/architecture/diagrams`.
