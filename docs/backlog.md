# Backlog

Цель проекта — AI Advent Challenge: активное развитие собственного AI-проекта.

Этот документ хранит список задач, которые предстоит реализовать в проекте.

## Формат
- `#` — крупные эпики.
- `##` — пользовательские истории или большие блоки работ.
- `- [ ]` — конкретные задачи, которые можно брать в работу.

## Wave 0 — базовая настройка проекта
### Backend (Java + Spring)
- [x] Код backend-проекта располагается в директории `backend/`.
- [x] Создать базовый Spring Boot сервис с REST контроллером-заглушкой.
- [x] Настроить конфигурацию через `application.yaml` + профили `local`/`prod`.
- [x] Настроить Gradle-проект с плагинами для проверок (`spotless`/`checkstyle` или `spotbugs`).
- [x] Реализовать базовые принципы разработки:
  - Использовать Clean Architecture: разделить слои controller/service/domain/persistence.
  - Включить здравый логгинг (`Slf4j`) и централизованный обработчик ошибок.
  - Писать модульные тесты с `JUnit 5` + `MockMvc`.
  - Для интеграционных тестов применять Testcontainers.
  - Описывать API в `OpenAPI/Swagger`.
- [x] Подготовить Dockerfile (OpenJDK 21 + слоистая сборка Spring).
- [x] Настроить миграции через Liquibase, интегрировать их в процесс сборки и деплоя.
- [x] Реализовать эндпоинт `/api/help`, который возвращает заглушку с текстовой подсказкой.
- [x] Зафиксировать, что все backend-эндпоинты публикуются с префиксом `/api` при доступе извне.

### Frontend
- [x] Код frontend-проекта располагается в директории `frontend/`.
- [x] Выбрать стек (варианты: `React + Vite`, `Vue + Vite`, `SvelteKit`); приоритизировать тот, что проще команде.
- [x] Сконфигурировать клиент для работы с REST API backend (axios/fetch с базовым клиентом).
- [x] Настроить компонентовую структуру (страница-заглушка + состояние подключения к backend).
- [x] Реализовать интеграцию: настроить окружение и эндпоинты для общения frontend ↔ backend, предусмотреть обработку ошибок и конфигурацию URL.
- [x] Добавить Dockerfile для frontend (node builder + static Nginx/serve слой).
- [x] Создать раздел `Help`, который обращается к `/api/help` и отображает результат; оставить заглушку на корневой странице.
- [x] Зафиксировать, что frontend работает на порту `4179` (в Docker и локально).

### Инфраструктура
- [x] Создать `docker-compose.yml`, который поднимает frontend и backend с нужными сетевыми алиасами.
- [x] Добавить сервис базы данных `postgres` c образом `pgvector/pgvector:pg15`, настроить volume для данных и переменные окружения.
- [x] Настроить `.env`/секреты для локальной разработки и деплоя.
- [x] Добавить GitHub Actions workflow: сборка backend, фронтенд, прогон тестов, билд контейнеров, деплой на VPS (например, через SSH + Docker Compose).
- [x] Документировать процесс запуска и деплоя в `README.md` или `docs/infra.md`.
- [x] Зафиксировать порты сервисов: backend — `8080`, frontend — `4179`; описать их в документации и `docker-compose.yml`.

## Wave 1 — интерактивный LLM-чат
### Backend
- [x] Подключить библиотеку Spring AI и настроить клиента для OpenAI-compatible провайдера z.ai (модель `glm-4.6`, базовый URL, ключ, параметры запросов).
- [x] Вынести настройки LLM (ключи, base URL, имя модели, опции стриминга) в `application.yaml` с возможностью переопределения через переменные окружения из `.env` и `docker-compose.yml`.
- [x] Добавить сервис домена чата, который проксирует запросы в LLM, обрабатывает стриминговые ответы (интерактивная передача токенов) и сохраняет историю диалогов в хранилище backend.
- [x] Реализовать SSE-эндпоинт на `/api/llm/chat/stream`, который принимает историю диалога и настройки запроса, стримит ответы LLM без дополнительной пост-обработки и не требует дополнительной авторизации.
- [x] Обновить OpenAPI/Swagger-описание и документацию (`docs/infra.md`/`README.md`) с инструкциями по запуску чата и используемым переменным окружения.

### Frontend
- [x] Добавить новую вкладку `LLM Chat` с формой отправки сообщений, списком истории диалога и подсветкой сообщений пользователя/модели.
- [x] Настроить подключение к SSE-эндпоинту backend, обрабатывать потоковые события, показывать токены по мере поступления и обрабатывать ошибки/разрывы соединения.
- [x] Обеспечить интерактивный ввод-вывод: отображать состояние запроса (loading/streaming), поддержать отмену запроса и повторную отправку без перезагрузки страницы.
- [x] Вынести адреса SSE и ключевые настройки фронтенда в конфигурацию окружения, синхронизировать переменные с `.env` и документацией.

### Тестирование и контроль качества
- [x] Добавить smoke-тесты/интеграционные проверки, покрывающие SSE-ручку и сохранение истории (например, MockMvc с Spring AI stub или контрактный тест).
- [x] Настроить минимальный e2e-сценарий или storybook/demo, чтобы проверить потоковый ответ от backend.
- [x] Обновить CI/CD сценарии при необходимости (секреты, прогон тестов).

#### Wave 1 gaps
- Покрытие SSE закрыто smoke и HTTP e2e тестами; дальнейшие улучшения можно выполнять по мере появления новых пользовательских сценариев.

**Решения и допущения**
- Провайдер LLM — z.ai, модель `glm-4.6`; настраиваем через совместимый OpenAI-клиент.
- SSE-эндпоинт открыт без дополнительной авторизации, опирается на существующую инфраструктуру защиты.
- История диалогов хранится на backend и доступна для восстановления контекста при следующих запросах.

## Wave 2 — расширенная документация
- [x] Спроектировать структуру каталога `docs/`:
  - `docs/overview.md` — цели проекта, ключевые понятия, дорожная карта.
  - `docs/architecture/` — отдельные файлы для backend (`backend.md`), frontend (`frontend.md`), интеграции с LLM (`llm.md`) и общих диаграмм (`diagrams/` с исходниками PlantUML/Excalidraw).
  - `docs/processes.md` — принципы разработки, тестирование, правила ведения документации (без инструкции по запуску).
  - `docs/faq.md` — типовые вопросы и сценарии использования.
- [x] Обновить существующие материалы (`docs/infra.md`, `README.md`) под новую структуру и связать перекрёстными ссылками.
- [x] Зафиксировать требования к обновлению документации при изменениях (чек-лист в `docs/CONTRIBUTING.md` или отдельный раздел).

## Wave 3 — мультипровайдерный чат и выбор моделей
### Цели
- [ ] Обеспечить возможность переключения между провайдерами z.ai и open.ai без изменения прикладного кода.
- [ ] Дать пользователю выбор набора поддерживаемых моделей от «эконом» до «топовых» внутри каждого провайдера.
- [ ] Стандартизировать создание и использование сущности `ChatClient` через выделенный сервис, уменьшая дублирование и связанные зависимости.

### Backend
- [ ] Вынести текущую логику построения клиента LLM в сервис-фабрику (`ChatClientFactory` или `ChatProviderService`), инкапсулируя настройки HTTP, стриминга и токенов.
- [ ] Добавить слой абстракции `ProviderStrategy`/`ProviderAdapter`, реализующий унифицированный интерфейс для z.ai и open.ai, включая различия в API и параметрах моделей.
- [ ] Расширить доменную модель чата: хранить в истории диалогов выбранный провайдер и модель, обновить миграции Liquibase.
- [ ] Обновить сервисы и контроллеры, чтобы при запросе можно было явно передать `provider` и `model`, с дефолтами на уровне конфигурации.
- [ ] Покрыть новую фабрику и стратегии модульными тестами; добавить интеграционный тест, проверяющий переключение провайдера (использовать стаб-реализации для каждого).

### Конфигурация и инфраструктура
- [ ] Описать в `application.yaml` и профилях структуру настроек провайдеров: base URL, ключ, таймауты, лимиты токенов, список доступных моделей (id, отображаемое имя, оценочная стоимость).
- [ ] Синхронизировать переменные окружения в `.env.example` и `docker-compose.yml`, обеспечить независимое управление ключами для обоих провайдеров.
- [ ] Обновить документацию (`docs/infra.md`, `README.md`) с таблицей моделей: класс (budget/standard/pro), ориентировочная стоимость и рекомендуемые сценарии.

### Frontend
- [ ] Добавить UI-компонент для выбора провайдера и модели перед отправкой запроса, автоподстройка доступных моделей в зависимости от выбранного провайдера.
- [ ] Отображать в интерфейсе текущий провайдер и модель в истории сообщений для упрощения отладки и поддержки.

### Тестирование и контроль качества
- [ ] Расширить e2e-сценарии чата: переключение провайдера и модели, проверка сохранения истории и корректности ответа.
- [ ] Настроить smoke-тесты в CI, которые выполняют запросы к обоим провайдерам с использованием моков или контрактов.

**Варианты подхода**
1. **Стратегия + фабрика (рекомендуется)** — единая `ChatClientFactory`, регистрирующая стратегии провайдеров через Spring (`@Component`/`@ConfigurationProperties`). Простой в сопровождении, позволяет быстро добавлять новые провайдеры, дружит с профилями и конфигурациями.
2. **Каталог провайдеров как конфиг** — хранить описания провайдеров и моделей в конфигурации или БД, динамически поднимать адаптеры на старте. Подходит, если планируется расширение списка провайдеров и нужен UI для управления.
3. **Унифицированный OpenAI-слой** — использовать один OpenAI-compatible клиент Spring AI с кастомизацией через `ClientCustomizer`. Минимальные изменения кода, но сложнее учесть несовместимости (стриминг, лимиты, специфичные параметры).
