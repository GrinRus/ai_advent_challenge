# Backlog

Цель проекта — AI Advent Challenge: активное развитие собственного AI-проекта.

Этот документ хранит список задач, которые предстоит реализовать в проекте.

## Формат
- `#` — крупные эпики.
- `##` — пользовательские истории или большие блоки работ.
- `- [ ]` — конкретные задачи, которые можно брать в работу.

## Wave 0 — базовая настройка проекта
### Backend (Java + Spring)
- [x] Код backend-проекта располагается в директории `backend/`.
- [x] Создать базовый Spring Boot сервис с REST контроллером-заглушкой.
- [x] Настроить конфигурацию через `application.yaml` + профили `local`/`prod`.
- [x] Настроить Gradle-проект с плагинами для проверок (`spotless`/`checkstyle` или `spotbugs`).
- [x] Реализовать базовые принципы разработки:
  - Использовать Clean Architecture: разделить слои controller/service/domain/persistence.
  - Включить здравый логгинг (`Slf4j`) и централизованный обработчик ошибок.
  - Писать модульные тесты с `JUnit 5` + `MockMvc`.
  - Для интеграционных тестов применять Testcontainers.
  - Описывать API в `OpenAPI/Swagger`.
- [x] Подготовить Dockerfile (OpenJDK 21 + слоистая сборка Spring).
- [x] Настроить миграции через Liquibase, интегрировать их в процесс сборки и деплоя.
- [x] Реализовать эндпоинт `/api/help`, который возвращает заглушку с текстовой подсказкой.
- [x] Зафиксировать, что все backend-эндпоинты публикуются с префиксом `/api` при доступе извне.

### Frontend
- [x] Код frontend-проекта располагается в директории `frontend/`.
- [x] Выбрать стек (варианты: `React + Vite`, `Vue + Vite`, `SvelteKit`); приоритизировать тот, что проще команде.
- [x] Сконфигурировать клиент для работы с REST API backend (axios/fetch с базовым клиентом).
- [x] Настроить компонентовую структуру (страница-заглушка + состояние подключения к backend).
- [x] Реализовать интеграцию: настроить окружение и эндпоинты для общения frontend ↔ backend, предусмотреть обработку ошибок и конфигурацию URL.
- [x] Добавить Dockerfile для frontend (node builder + static Nginx/serve слой).
- [x] Создать раздел `Help`, который обращается к `/api/help` и отображает результат; оставить заглушку на корневой странице.
- [x] Зафиксировать, что frontend работает на порту `4179` (в Docker и локально).

### Инфраструктура
- [x] Создать `docker-compose.yml`, который поднимает frontend и backend с нужными сетевыми алиасами.
- [x] Добавить сервис базы данных `postgres` c образом `pgvector/pgvector:pg15`, настроить volume для данных и переменные окружения.
- [x] Настроить `.env`/секреты для локальной разработки и деплоя.
- [x] Добавить GitHub Actions workflow: сборка backend, фронтенд, прогон тестов, билд контейнеров, деплой на VPS (например, через SSH + Docker Compose).
- [x] Документировать процесс запуска и деплоя в `README.md` или `docs/infra.md`.
- [x] Зафиксировать порты сервисов: backend — `8080`, frontend — `4179`; описать их в документации и `docker-compose.yml`.

## Wave 1 — интерактивный LLM-чат
### Backend
- [x] Подключить библиотеку Spring AI и настроить клиента для OpenAI-compatible провайдера z.ai (модель `glm4.6`, базовый URL, ключ, параметры запросов).
- [x] Вынести настройки LLM (ключи, base URL, имя модели, опции стриминга) в `application.yaml` с возможностью переопределения через переменные окружения из `.env` и `docker-compose.yml`.
- [x] Добавить сервис домена чата, который проксирует запросы в LLM, обрабатывает стриминговые ответы (интерактивная передача токенов) и сохраняет историю диалогов в хранилище backend.
- [x] Реализовать SSE-эндпоинт на `/api/llm/chat/stream`, который принимает историю диалога и настройки запроса, стримит ответы LLM без дополнительной пост-обработки и не требует дополнительной авторизации.
- [x] Обновить OpenAPI/Swagger-описание и документацию (`docs/infra.md`/`README.md`) с инструкциями по запуску чата и используемым переменным окружения.

### Frontend
- [x] Добавить новую вкладку `LLM Chat` с формой отправки сообщений, списком истории диалога и подсветкой сообщений пользователя/модели.
- [x] Настроить подключение к SSE-эндпоинту backend, обрабатывать потоковые события, показывать токены по мере поступления и обрабатывать ошибки/разрывы соединения.
- [x] Обеспечить интерактивный ввод-вывод: отображать состояние запроса (loading/streaming), поддержать отмену запроса и повторную отправку без перезагрузки страницы.
- [x] Вынести адреса SSE и ключевые настройки фронтенда в конфигурацию окружения, синхронизировать переменные с `.env` и документацией.

### Тестирование и контроль качества
- [ ] Добавить smoke-тесты/интеграционные проверки, покрывающие SSE-ручку и сохранение истории (например, MockMvc с Spring AI stub или контрактный тест).
- [ ] Настроить минимальный e2e-сценарий или storybook/demo, чтобы проверить потоковый ответ от backend.
- [ ] Обновить CI/CD сценарии при необходимости (секреты, прогон тестов).

#### Wave 1 gaps
- Не реализованы автоматические проверки: отсутствуют smoke/интеграционные тесты для SSE и сохранения истории, нет e2e/storybook-демо и не пересмотрен CI/CD пайплайн под новые сценарии.

**Решения и допущения**
- Провайдер LLM — z.ai, модель `glm4.6`; настраиваем через совместимый OpenAI-клиент.
- SSE-эндпоинт открыт без дополнительной авторизации, опирается на существующую инфраструктуру защиты.
- История диалогов хранится на backend и доступна для восстановления контекста при следующих запросах.
